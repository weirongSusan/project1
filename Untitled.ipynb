{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section must be included at the beginning of each new notebook. Remember to change the app name. \n",
    "# If you're using VirtualBox, change the below to '/home/user/spark-2.1.1-bin-hadoop2.7'\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in the data. If you open the dataset, you'll find that each column has a header. We specify that by stating that header=True.\n",
    "# To make our lives easier, we can also use 'inferSchema' when importing CSVs. This automatically detects data types.\n",
    "# If you would like to manually change data types, refer to this article: https://medium.com/@mrpowers/adding-structtype-columns-to-spark-dataframes-b44125409803\n",
    "df = spark.read.csv('absenteeism3.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+----------------+---------------+-------+----------------------+-------------------------------+------------+---+----------------------+----------+--------------------+---------+---+--------------+-------------+---+------+---------------+-------------------------+------+\n",
      "| ID|Reason for absence|Month of absence|Day of the week|Seasons|Transportation expense|Distance from Residence to Work|Service time|Age|Work load Average/day |Hit target|Disciplinary failure|Education|Son|Social drinker|Social smoker|Pet|Weight|Body mass index|Absenteeism time in hours|Height|\n",
      "+---+------------------+----------------+---------------+-------+----------------------+-------------------------------+------------+---+----------------------+----------+--------------------+---------+---+--------------+-------------+---+------+---------------+-------------------------+------+\n",
      "| 11|                26|               7|              3|      1|                   289|                             36|          13| 33|               239,554|        97|                   0|        1|  2|             1|            0|  1|    90|             30|                        4|  1.72|\n",
      "| 36|                 0|               7|              3|      1|                   118|                             13|          18| 50|               239,554|        97|                   1|        1|  1|             1|            0|  0|    98|             31|                        0|  1.78|\n",
      "|  3|                23|               7|              4|      1|                   179|                             51|          18| 38|               239,554|        97|                   0|        1|  0|             1|            0|  0|    89|             31|                        2|   1.7|\n",
      "|  7|                 7|               7|              5|      1|                   279|                              5|          14| 39|               239,554|        97|                   0|        1|  2|             1|            1|  0|    68|             24|                        4|  1.68|\n",
      "| 11|                23|               7|              5|      1|                   289|                             36|          13| 33|               239,554|        97|                   0|        1|  2|             1|            0|  1|    90|             30|                        2|  1.72|\n",
      "|  3|                23|               7|              6|      1|                   179|                             51|          18| 38|               239,554|        97|                   0|        1|  0|             1|            0|  0|    89|             31|                        2|   1.7|\n",
      "| 10|                22|               7|              6|      1|                   361|                             52|           3| 28|               239,554|        97|                   0|        1|  1|             1|            0|  4|    80|             27|                        8|  1.72|\n",
      "| 20|                23|               7|              6|      1|                   260|                             50|          11| 36|               239,554|        97|                   0|        1|  4|             1|            0|  0|    65|             23|                        4|  1.68|\n",
      "| 14|                19|               7|              2|      1|                   155|                             12|          14| 34|               239,554|        97|                   0|        1|  2|             1|            0|  0|    95|             25|                       40|  1.96|\n",
      "|  1|                22|               7|              2|      1|                   235|                             11|          14| 37|               239,554|        97|                   0|        3|  1|             0|            0|  1|    88|             29|                        8|  1.72|\n",
      "| 20|                 1|               7|              2|      1|                   260|                             50|          11| 36|               239,554|        97|                   0|        1|  4|             1|            0|  0|    65|             23|                        8|  1.68|\n",
      "| 20|                 1|               7|              3|      1|                   260|                             50|          11| 36|               239,554|        97|                   0|        1|  4|             1|            0|  0|    65|             23|                        8|  1.68|\n",
      "| 20|                11|               7|              4|      1|                   260|                             50|          11| 36|               239,554|        97|                   0|        1|  4|             1|            0|  0|    65|             23|                        8|  1.68|\n",
      "|  3|                11|               7|              4|      1|                   179|                             51|          18| 38|               239,554|        97|                   0|        1|  0|             1|            0|  0|    89|             31|                        1|   1.7|\n",
      "|  3|                23|               7|              4|      1|                   179|                             51|          18| 38|               239,554|        97|                   0|        1|  0|             1|            0|  0|    89|             31|                        4|   1.7|\n",
      "| 24|                14|               7|              6|      1|                   246|                             25|          16| 41|               239,554|        97|                   0|        1|  0|             1|            0|  0|    67|             23|                        8|   1.7|\n",
      "|  3|                23|               7|              6|      1|                   179|                             51|          18| 38|               239,554|        97|                   0|        1|  0|             1|            0|  0|    89|             31|                        2|   1.7|\n",
      "|  3|                21|               7|              2|      1|                   179|                             51|          18| 38|               239,554|        97|                   0|        1|  0|             1|            0|  0|    89|             31|                        8|   1.7|\n",
      "|  6|                11|               7|              5|      1|                   189|                             29|          13| 33|               239,554|        97|                   0|        1|  2|             0|            0|  2|    69|             25|                        8|  1.67|\n",
      "| 33|                23|               8|              4|      1|                   248|                             25|          14| 47|               205,917|        92|                   0|        1|  2|             0|            0|  1|    86|             32|                        2|  1.65|\n",
      "+---+------------------+----------------+---------------+-------+----------------------+-------------------------------+------------+---+----------------------+----------+--------------------+---------+---+--------------+-------------+---+------+---------------+-------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The show method allows you visualise DataFrames in a tabular format. \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Reason for absence: integer (nullable = true)\n",
      " |-- Month of absence: integer (nullable = true)\n",
      " |-- Day of the week: integer (nullable = true)\n",
      " |-- Seasons: integer (nullable = true)\n",
      " |-- Transportation expense: integer (nullable = true)\n",
      " |-- Distance from Residence to Work: integer (nullable = true)\n",
      " |-- Service time: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Work load Average/day : string (nullable = true)\n",
      " |-- Hit target: integer (nullable = true)\n",
      " |-- Disciplinary failure: integer (nullable = true)\n",
      " |-- Education: integer (nullable = true)\n",
      " |-- Son: integer (nullable = true)\n",
      " |-- Social drinker: integer (nullable = true)\n",
      " |-- Social smoker: integer (nullable = true)\n",
      " |-- Pet: integer (nullable = true)\n",
      " |-- Weight: integer (nullable = true)\n",
      " |-- Body mass index: integer (nullable = true)\n",
      " |-- Absenteeism time in hours: integer (nullable = true)\n",
      " |-- Height: double (nullable = true)\n",
      "\n",
      "[Row(ID=11, Reason for absence=26, Month of absence=7, Day of the week=3, Seasons=1, Transportation expense=289, Distance from Residence to Work=36, Service time=13, Age=33, Work load Average/day ='239,554', Hit target=97, Disciplinary failure=0, Education=1, Son=2, Social drinker=1, Social smoker=0, Pet=1, Weight=90, Body mass index=30, Absenteeism time in hours=4, Height=1.72)]\n"
     ]
    }
   ],
   "source": [
    "# Print schema allows us to visualise the data structure at a high level. \n",
    "df.printSchema()\n",
    "\n",
    "# We can also use head to print a specific amount of rows, so we can get a better understanding of the data points. \n",
    "# Note that we have to specify 'print' depending on the method we're using. Otherwise it may not show up!\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+----------------------+-------------------------------+------------------+-----------------+----------------------+-----------------+--------------------+------------------+------------------+-------------------+-------------------+------------------+------------------+------------------+-------------------------+--------------------+\n",
      "|summary|                ID|Reason for absence|  Month of absence|   Day of the week|           Seasons|Transportation expense|Distance from Residence to Work|      Service time|              Age|Work load Average/day |       Hit target|Disciplinary failure|         Education|               Son|     Social drinker|      Social smoker|               Pet|            Weight|   Body mass index|Absenteeism time in hours|              Height|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+----------------------+-------------------------------+------------------+-----------------+----------------------+-----------------+--------------------+------------------+------------------+-------------------+-------------------+------------------+------------------+------------------+-------------------------+--------------------+\n",
      "|  count|               740|               740|               740|               740|               740|                   740|                            740|               740|              740|                   740|              740|                 740|               740|               740|                740|                740|               740|               740|               740|                      740|                 740|\n",
      "|   mean|18.017567567567568|19.216216216216218| 6.324324324324325|3.9148648648648647|2.5445945945945945|    221.32972972972973|              29.63108108108108|12.554054054054054|            36.45|                  null|94.58783783783784| 0.05405405405405406|1.2918918918918918| 1.018918918918919| 0.5675675675675675|0.07297297297297298| 0.745945945945946| 79.03513513513514|26.677027027027027|        6.924324324324324|  1.7211486486486636|\n",
      "| stddev|11.021247263063657|  8.43340588279965|3.4362869319125897|1.4216747097562805| 1.111831060157382|     66.95222324531973|             14.836788436739143| 4.384873407621148|6.478772457611868|                  null|3.779313134418015| 0.22627727323215058|0.6732380415251598|1.0984890195302817|0.49574866720003496| 0.2602680502800183|1.3182582913258336|12.883210507177214| 4.285452223167274|       13.330998100978196|0.060349945302676636|\n",
      "|    min|                 1|                 0|                 0|                 2|                 1|                   118|                              5|                 1|               27|               205,917|               81|                   0|                 1|                 0|                  0|                  0|                 0|                56|                19|                        0|                1.63|\n",
      "|    max|                36|                28|                12|                 6|                 4|                   388|                             52|                29|               58|               378,884|              100|                   1|                 4|                 4|                  1|                  1|                 8|               108|                38|                      120|                1.96|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+----------------------+-------------------------------+------------------+-----------------+----------------------+-----------------+--------------------+------------------+------------------+-------------------+-------------------+------------------+------------------+------------------+-------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can use the describe method get some general statistics on our data too. \n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+---+---+---+------+---------------+-------------------------+------+\n",
      "| ID|Reason for absence|age|Son|Pet|Weight|Body mass index|Absenteeism time in hours|Height|\n",
      "+---+------------------+---+---+---+------+---------------+-------------------------+------+\n",
      "| 11|                26| 33|  2|  1|    90|             30|                        4|  1.72|\n",
      "| 36|                 0| 50|  1|  0|    98|             31|                        0|  1.78|\n",
      "|  3|                23| 38|  0|  0|    89|             31|                        2|   1.7|\n",
      "|  7|                 7| 39|  2|  0|    68|             24|                        4|  1.68|\n",
      "| 11|                23| 33|  2|  1|    90|             30|                        2|  1.72|\n",
      "|  3|                23| 38|  0|  0|    89|             31|                        2|   1.7|\n",
      "| 10|                22| 28|  1|  4|    80|             27|                        8|  1.72|\n",
      "| 20|                23| 36|  4|  0|    65|             23|                        4|  1.68|\n",
      "| 14|                19| 34|  2|  0|    95|             25|                       40|  1.96|\n",
      "|  1|                22| 37|  1|  1|    88|             29|                        8|  1.72|\n",
      "| 20|                 1| 36|  4|  0|    65|             23|                        8|  1.68|\n",
      "| 20|                 1| 36|  4|  0|    65|             23|                        8|  1.68|\n",
      "| 20|                11| 36|  4|  0|    65|             23|                        8|  1.68|\n",
      "|  3|                11| 38|  0|  0|    89|             31|                        1|   1.7|\n",
      "|  3|                23| 38|  0|  0|    89|             31|                        4|   1.7|\n",
      "| 24|                14| 41|  0|  0|    67|             23|                        8|   1.7|\n",
      "|  3|                23| 38|  0|  0|    89|             31|                        2|   1.7|\n",
      "|  3|                21| 38|  0|  0|    89|             31|                        8|   1.7|\n",
      "|  6|                11| 33|  2|  2|    69|             25|                        8|  1.67|\n",
      "| 33|                23| 47|  2|  1|    86|             32|                        2|  1.65|\n",
      "+---+------------------+---+---+---+------+---------------+-------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's select the balance column and assign it to a variable. \n",
    "df= df.select('ID', 'Reason for absence', 'age', 'Son', 'Pet', 'Weight', 'Body mass index', 'Absenteeism time in hours', 'Height')\n",
    "\n",
    "# We can then use the show method on that variable.\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "# If you're using VirtualBox, change the below to '/home/user/spark-2.1.1-bin-hadoop2.7'\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('linear_regression_docs').getOrCreate()\n",
    "\n",
    "# If you're getting an error with numpy, please type 'sudo pip3 install numpy --user' into the console.\n",
    "# If you're getting an error with another package, type 'sudo pip3 install PACKAGENAME --user'. \n",
    "# Replace PACKAGENAME with the relevant package (such as pandas, etc).\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o155.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 9, localhost, executor driver): java.lang.NumberFormatException: For input string: \"ID,Reason\"\n\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.lang.Double.parseDouble(Double.java:538)\n\tat scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:29)\n\tat org.apache.spark.mllib.util.MLUtils$.parseLibSVMRecord(MLUtils.scala:107)\n\tat org.apache.spark.mllib.util.MLUtils$$anonfun$parseLibSVMFile$3.apply(MLUtils.scala:102)\n\tat org.apache.spark.mllib.util.MLUtils$$anonfun$parseLibSVMFile$3.apply(MLUtils.scala:102)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1012)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1010)\n\tat org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1987)\n\tat org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1987)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1988)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\n\tat org.apache.spark.mllib.util.MLUtils$.computeNumFeatures(MLUtils.scala:92)\n\tat org.apache.spark.ml.source.libsvm.LibSVMFileFormat$$anonfun$2.apply$mcI$sp(LibSVMRelation.scala:112)\n\tat org.apache.spark.ml.source.libsvm.LibSVMFileFormat$$anonfun$2.apply(LibSVMRelation.scala:99)\n\tat org.apache.spark.ml.source.libsvm.LibSVMFileFormat$$anonfun$2.apply(LibSVMRelation.scala:99)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.ml.source.libsvm.LibSVMFileFormat.inferSchema(LibSVMRelation.scala:99)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:184)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:184)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$getOrInferFileFormatSchema(DataSource.scala:183)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:387)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:152)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:135)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NumberFormatException: For input string: \"ID,Reason\"\n\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.lang.Double.parseDouble(Double.java:538)\n\tat scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:29)\n\tat org.apache.spark.mllib.util.MLUtils$.parseLibSVMRecord(MLUtils.scala:107)\n\tat org.apache.spark.mllib.util.MLUtils$$anonfun$parseLibSVMFile$3.apply(MLUtils.scala:102)\n\tat org.apache.spark.mllib.util.MLUtils$$anonfun$parseLibSVMFile$3.apply(MLUtils.scala:102)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1012)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1010)\n\tat org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1987)\n\tat org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1987)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1ff155605682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load model training data. Location of the data may be different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"libsvm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"absenteeism3.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o155.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 9, localhost, executor driver): java.lang.NumberFormatException: For input string: \"ID,Reason\"\n\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.lang.Double.parseDouble(Double.java:538)\n\tat scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:29)\n\tat org.apache.spark.mllib.util.MLUtils$.parseLibSVMRecord(MLUtils.scala:107)\n\tat org.apache.spark.mllib.util.MLUtils$$anonfun$parseLibSVMFile$3.apply(MLUtils.scala:102)\n\tat org.apache.spark.mllib.util.MLUtils$$anonfun$parseLibSVMFile$3.apply(MLUtils.scala:102)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1012)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1010)\n\tat org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1987)\n\tat org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1987)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1988)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\n\tat org.apache.spark.mllib.util.MLUtils$.computeNumFeatures(MLUtils.scala:92)\n\tat org.apache.spark.ml.source.libsvm.LibSVMFileFormat$$anonfun$2.apply$mcI$sp(LibSVMRelation.scala:112)\n\tat org.apache.spark.ml.source.libsvm.LibSVMFileFormat$$anonfun$2.apply(LibSVMRelation.scala:99)\n\tat org.apache.spark.ml.source.libsvm.LibSVMFileFormat$$anonfun$2.apply(LibSVMRelation.scala:99)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.ml.source.libsvm.LibSVMFileFormat.inferSchema(LibSVMRelation.scala:99)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:184)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:184)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$getOrInferFileFormatSchema(DataSource.scala:183)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:387)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:152)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:135)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NumberFormatException: For input string: \"ID,Reason\"\n\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.lang.Double.parseDouble(Double.java:538)\n\tat scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:29)\n\tat org.apache.spark.mllib.util.MLUtils$.parseLibSVMRecord(MLUtils.scala:107)\n\tat org.apache.spark.mllib.util.MLUtils$$anonfun$parseLibSVMFile$3.apply(MLUtils.scala:102)\n\tat org.apache.spark.mllib.util.MLUtils$$anonfun$parseLibSVMFile$3.apply(MLUtils.scala:102)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1012)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1010)\n\tat org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1987)\n\tat org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1987)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Load model training data. Location of the data may be different.\n",
    "training = spark.read.format(\"libsvm\").load(\"absenteeism3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
